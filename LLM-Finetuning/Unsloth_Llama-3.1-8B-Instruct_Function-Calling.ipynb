{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01ba05a5aa444b458159e94115a191dc"
   },
   "source": [
    "## Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9a3ff32f-e0b3-45d1-a52b-f4b98562332e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.13\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch==2.5.1 transformers==4.46.2 datasets wandb huggingface_hub python-dotenv --no-cache-dir | tail -n 1 \n",
    "# !pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "# !pip install --no-deps trl peft accelerate bitsandbytes xformers==0.0.28.post3 --no-cache | tail -n 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "425d4162b68448358dc4dcc502d982be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cu121\n",
      "CUDA available: True\n",
      "Number of GPUs: 1\n",
      "GPU Name: NVIDIA L40S\n",
      "Current GPU Device: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Print Torch version (greater than 2.4 for xformers)\n",
    "print(torch.__version__)\n",
    "\n",
    "# Check if CUDA (GPU) is available\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "# Print the number of GPUs\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "\n",
    "# Print the name of the current GPU device\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# Print the current GPU device index\n",
    "print(\"Current GPU Device:\", torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ceee60966044c448b9a58cec75da649"
   },
   "source": [
    "## Setting Up Weights & Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8aafcf9921c74caf84dfc7f92a64b6ed"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgautamgc75\u001b[0m (\u001b[33mgautamgc75-org\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/wsuser/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully logged into WandB.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/wsuser/work/wandb/run-20241103_064129-z0tfh4jg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gautamgc75-org/llama-3.1-8B-ft-lora-fc/runs/z0tfh4jg' target=\"_blank\">run-v1</a></strong> to <a href='https://wandb.ai/gautamgc75-org/llama-3.1-8B-ft-lora-fc' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gautamgc75-org/llama-3.1-8B-ft-lora-fc' target=\"_blank\">https://wandb.ai/gautamgc75-org/llama-3.1-8B-ft-lora-fc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gautamgc75-org/llama-3.1-8B-ft-lora-fc/runs/z0tfh4jg' target=\"_blank\">https://wandb.ai/gautamgc75-org/llama-3.1-8B-ft-lora-fc/runs/z0tfh4jg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandB run initialized: Project - llama-3.1-8B-ft-lora-fc, Run - run-v1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "def setup_wandb(project_name: str, run_name: str):\n",
    "    # Set up your API KEY\n",
    "    try:\n",
    "        api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "        wandb.login(key=api_key)\n",
    "        print(\"Successfully logged into WandB.\")\n",
    "    except KeyError:\n",
    "        raise EnvironmentError(\"WANDB_API_KEY is not set in the environment variables.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error logging into WandB: {e}\")\n",
    "    \n",
    "    # Optional: Log models\n",
    "    os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"\n",
    "    os.environ[\"WANDB_WATCH\"] = \"all\"\n",
    "    os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "    \n",
    "    # Initialize the WandB run\n",
    "    try:\n",
    "        wandb.init(project=project_name, name=run_name)\n",
    "        print(f\"WandB run initialized: Project - {project_name}, Run - {run_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing WandB run: {e}\")\n",
    "\n",
    "# Run it as a function\n",
    "setup_wandb(project_name=\"llama-3.1-8B-ft-lora-fc\", run_name=\"run-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6eda924a170d47388b3c1f3bea9b4455"
   },
   "source": [
    "## HuggingFace Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0bdd801695b24995bad394b148845718"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "if hf_token is None:\n",
    "    raise EnvironmentError(\"HUGGINGFACE_TOKEN is not set in the environment variables.\")\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef17a085615d4fe6853723f57feb4e70"
   },
   "source": [
    "## Loading the Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d12bc5c17aac45798bbb6b781b59d458"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 2048     # Unsloth auto supports RoPE Scaling internally!\n",
    "dtype = None              # None for auto detection\n",
    "load_in_4bit = False      # Use 4bit quantization to reduce memory usage. \n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct\",  \n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ae0b9e43fdc642bc89c40187f7d4f7a2"
   },
   "source": [
    "## Configuring LoRA for Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "65c6dbf7f663434d8f71dad96d8f545d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.10.7 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,              # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0,    # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",       # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = \"unsloth\",     # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1e2feb0804b245f693f2bbce54d66e96"
   },
   "source": [
    "## Loading and Processing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "87338b045414408b88a30f90d2f683b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a sample size of 20000 for fine-tuning.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Loading the dataset\n",
    "dataset = load_dataset(\"Salesforce/xlam-function-calling-60k\", split=\"train\", token=hf_token)\n",
    "\n",
    "# Selecting a subset of 15K samples for fine-tuning\n",
    "dataset = dataset.select(range(20000))\n",
    "print(f\"Using a sample size of {len(dataset)} for fine-tuning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "b95a8a9038274efb8f4dcdd74ba03176"
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "# Initialize the tokenizer with the chat template and mapping\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3\",  # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
    "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
    "    map_eos_token = True,        # Maps <|im_end|> to <|eot_id|> instead\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = []\n",
    "    \n",
    "    # Iterate through each item in the batch (examples are structured as lists of values)\n",
    "    for query, tools, answers in zip(examples['query'], examples['tools'], examples['answers']):\n",
    "        tool_user = {\n",
    "            \"content\": f\"You are a helpful assistant with access to the following tools or function calls. Your task is to produce a sequence of tools or function calls necessary to generate response to the user utterance. Use the following tools or function calls as required:\\n{tools}\",\n",
    "            \"role\": \"system\"\n",
    "        }\n",
    "        ques_user = {\n",
    "            \"content\": f\"{query}\",\n",
    "            \"role\": \"user\"\n",
    "        }\n",
    "        assistant = {\n",
    "            \"content\": f\"{answers}\",\n",
    "            \"role\": \"assistant\"\n",
    "        }\n",
    "\n",
    "        # Combine the user1, user2, and assistant into a single conversation structure\n",
    "        convos.append([tool_user, ques_user, assistant])\n",
    "\n",
    "    # Apply the chat template tokenizer for each conversation\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in convos]\n",
    "    return {\"text\": texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "20ffbd4bf2834b8b8b8540d77c8777b2"
   },
   "outputs": [],
   "source": [
    "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "d4a538f64e99453785528021418aec41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful assistant with access to the following tools or function calls. Your task is to produce a sequence of tools or function calls necessary to generate response to the user utterance. Use the following tools or function calls as required:\n",
      "[{\"name\": \"live_giveaways_by_type\", \"description\": \"Retrieve live giveaways from the GamerPower API based on the specified type.\", \"parameters\": {\"type\": {\"description\": \"The type of giveaways to retrieve (e.g., game, loot, beta).\", \"type\": \"str\", \"default\": \"game\"}}}]<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Where can I find live giveaways for beta access and games?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "[{\"name\": \"live_giveaways_by_type\", \"arguments\": {\"type\": \"beta\"}}, {\"name\": \"live_giveaways_by_type\", \"arguments\": {\"type\": \"game\"}}]<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fffae6c161f4fcf85e6764c1b85c3a5"
   },
   "source": [
    "## Training with SFTTrainer and Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3014144a0d1e46f090b0f508d96f4df6"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "        per_device_train_batch_size = 4,\n",
    "        gradient_accumulation_steps = 2,    \n",
    "        warmup_steps = 5,\n",
    "        learning_rate = 2e-4,\n",
    "        num_train_epochs = 3,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"wandb\",  \n",
    "        logging_steps = 1,  \n",
    "        logging_strategy = \"steps\",\n",
    "        save_strategy = \"no\",\n",
    "        load_best_model_at_end = True,\n",
    "        save_only_model = False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bbd110c517bc4eb39b683bbce4a385dd"
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False,       \n",
    "    args = args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "5709e32b5fab422b826c33350300d9f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA L40S. Max memory = 44.521 GB.\n",
      "15.887 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# Show current memory stats\n",
    "\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "7ee949eaf38b41d88faecb1117aecd25"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "807"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Clear PyTorch's cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Run Python's garbage collector\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "79d7a9f294ab416283267aba94ed25e0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 20,000 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 2\n",
      "\\        /    Total batch size = 8 | Total steps = 7,500\n",
      " \"-____-\"     Number of trainable parameters = 41,943,040\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   2/7500 : < :, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import unsloth_train\n",
    "trainer_stats = unsloth_train(trainer)   # << Fixed gradient accumulation\n",
    "# trainer_stats = trainer.train()        # << Buggy if using gradient accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d11b6a4baced4ecb82134c74e374f83f"
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "253f030aa6f24e75a7eb25d17815e527"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainOutput(global_step=7500, training_loss=0.14057622442046802, metrics={'train_runtime': 18099.7207, 'train_samples_per_second': 3.315, 'train_steps_per_second': 0.414, 'total_flos': 1.8451314530340372e+18, 'train_loss': 0.14057622442046802, 'epoch': 3.0})\n"
     ]
    }
   ],
   "source": [
    "print(trainer_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "1fd6d058bdf249d183efff41e7c5aa53"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_runtime': 18099.7207,\n",
       " 'train_samples_per_second': 3.315,\n",
       " 'train_steps_per_second': 0.414,\n",
       " 'total_flos': 1.8451314530340372e+18,\n",
       " 'train_loss': 0.14057622442046802,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_stats.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "6b8d974ec401448d9f5ee6808fc1fe0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18099.7207 seconds used for training.\n",
      "301.66 minutes used for training.\n",
      "Peak reserved memory = 39.855 GB.\n",
      "Peak reserved memory for training = 23.968 GB.\n",
      "Peak reserved memory % of max memory = 89.52 %.\n",
      "Peak reserved memory for training % of max memory = 53.835 %.\n"
     ]
    }
   ],
   "source": [
    "# Show final memory and time stats\n",
    "\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74317edc9fb7439e829daf26c730d7a9"
   },
   "source": [
    "## Saving the Model\n",
    "\n",
    "After training, the fine-tuned model is saved locally and pushed to Hugging Face's hub for further access and deployment. However, this only saves the LoRA adapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ad3a42a66364bda93d7b8168258a691"
   },
   "outputs": [],
   "source": [
    "# Local saving\n",
    "model.save_pretrained(\"Llama3-FineTune-Function-Calling-LoRA-Model-V2\") \n",
    "tokenizer.save_pretrained(\"Llama3-FineTune-Function-Calling-LoRA-Model-V2\")\n",
    "\n",
    "# Online saving\n",
    "model.push_to_hub(\"gautamgc17/Llama3-FineTune-Function-Calling-LoRA-Model-V2\", token = hf_token)\n",
    "tokenizer.push_to_hub(\"gautamgc17/Llama3-FineTune-Function-Calling-LoRA-Model-V2\", token = hf_token) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e959b4a4e84640808ee90780ccf7e1f3"
   },
   "source": [
    "## Saving to float16 for VLLM\n",
    "For merging the LoRA adapters with the base model and save the model to 16-bit precision for optimized performance with vLLM, use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "984d3e3c7f2745ef89163af42128ba4e"
   },
   "outputs": [],
   "source": [
    "# Merge to 16bit\n",
    "model.save_pretrained_merged(\"Llama3-FineTune-Function-Calling-Model-V2\", tokenizer, save_method = \"merged_16bit\")\n",
    "model.push_to_hub_merged(\"gautamgc17/Llama3-FineTune-Function-Calling-Model-V2\", tokenizer, save_method = \"merged_16bit\", token = hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7c0e1f4d9ef41538fdedb5f2cbb536b"
   },
   "source": [
    "## Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1096effdb2e0421d900bc0d7d7e9bbde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "==((====))==  Unsloth 2024.10.7: Fast Llama patching. Transformers = 4.46.1.\n",
      "   \\\\   /|    GPU: NVIDIA L40S. Max memory: 44.521 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.1+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "756cfb17cc864a968d0564a57f28a02c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaExtendedRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from transformers import TextStreamer\n",
    "\n",
    "max_seq_length = 2048     \n",
    "dtype = None              \n",
    "load_in_4bit = False     \n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"Llama3-FineTune-Function-Calling-Model-V2\",  \n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)   # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "6152a4a72c134feea408e0fde4d128dd"
   },
   "outputs": [],
   "source": [
    "WEATHER_API_KEY = os.getenv(\"WEATHER_API_KEY\")\n",
    "NASA_API_KEY = os.getenv(\"NASA_API_KEY\")\n",
    "STOCK_API_KEY = os.getenv(\"STOCK_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "d7f0ad17eabc4650813e610c53fd34d3"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import nasapy\n",
    "\n",
    "\n",
    "def get_current_date() -> str:\n",
    "    \"\"\"\n",
    "    Fetches the current date in the format YYYY-MM-DD.\n",
    "    Returns:\n",
    "        str: A string representing the current date.\n",
    "    \"\"\"\n",
    "    print(\"Getting the current date\")\n",
    "    \n",
    "    try:\n",
    "        current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        return current_date\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching current date: {e}\")\n",
    "        return \"NA\"\n",
    "    \n",
    "    \n",
    "def get_current_weather(location: str) -> dict:\n",
    "    \"\"\"\n",
    "    Fetches the current weather for a given location (default: San Francisco).\n",
    "    Args:\n",
    "        location (str): The name of the city for which to retrieve the weather information.\n",
    "    Returns:\n",
    "        dict: A dictionary containing weather information such as temperature, weather description, and humidity.\n",
    "    \"\"\"\n",
    "    print(f\"Getting current weather for {location}\")\n",
    "    \n",
    "    try:\n",
    "        weather_url = f\"http://api.openweathermap.org/data/2.5/weather?q={location}&appid={WEATHER_API_KEY}&units=metric\"\n",
    "        weather_data = requests.get(weather_url)\n",
    "        data = weather_data.json()\n",
    "        weather_description = data[\"weather\"][0][\"description\"]\n",
    "        temperature = data[\"main\"][\"temp\"]\n",
    "        humidity = data[\"main\"][\"humidity\"]\n",
    "        return {\n",
    "            \"description\": weather_description,\n",
    "            \"temperature\": temperature,\n",
    "            \"humidity\": humidity\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching weather data: {e}\")\n",
    "        return {\"weather\": \"NA\"}\n",
    "    \n",
    "    \n",
    "def celsius_to_fahrenheit(celsius: float):\n",
    "    \"\"\"\n",
    "    Converts a temperature from Celsius to Fahrenheit.\n",
    "    \n",
    "    Args:\n",
    "        celsius (float): Temperature in degrees Celsius.\n",
    "        \n",
    "    Returns:\n",
    "        float: Temperature in degrees Fahrenheit.\n",
    "    \"\"\"\n",
    "    print(f\"Converting {celsius}°C to Fahrenheit\")\n",
    "    \n",
    "    try:\n",
    "        fahrenheit = (celsius * 9/5) + 32\n",
    "        return fahrenheit\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting temperature: {e}\")\n",
    "        return None\n",
    "    \n",
    "    \n",
    "def get_nasa_picture_of_the_day(date: str) -> dict:\n",
    "    \"\"\"\n",
    "    Fetches NASA's Picture of the Day information for a given date.\n",
    "    \n",
    "    Args:\n",
    "        date (str): The date for which to retrieve the picture in 'YYYY-MM-DD' format.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing the title, explanation, and URL of the image or video.\n",
    "    \"\"\"\n",
    "    print(f\"Getting NASA's Picture of the Day for {date}\")\n",
    "    \n",
    "    try:\n",
    "        nasa = nasapy.Nasa(key = NASA_API_KEY)\n",
    "        apod = nasa.picture_of_the_day(date = date, hd=True)\n",
    "        title = apod.get(\"title\", \"No Title\")\n",
    "        explanation = apod.get(\"explanation\", \"No Explanation\")\n",
    "        url = apod.get(\"url\", \"No URL\")\n",
    "        return {\n",
    "            \"title\": title,\n",
    "            \"explanation\": explanation,\n",
    "            \"url\": url\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching NASA's Picture of the Day: {e}\")\n",
    "        return {\"error\": \"Unable to fetch NASA Picture of the Day\"}\n",
    "    \n",
    "    \n",
    "def get_stock_price(ticker: str, date: str) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Retrieves the lowest and highest stock prices for a given ticker and date.\n",
    "    Args:\n",
    "        ticker (str): The stock ticker symbol, e.g., \"IBM\".\n",
    "        date (str): The date in \"YYYY-MM-DD\" format for which you want to get stock prices.\n",
    "    Returns:\n",
    "        tuple: A tuple containing the low and high stock prices on the given date, or (\"none\", \"none\") if not found.\n",
    "    \"\"\"\n",
    "    print(f\"Getting stock price for {ticker} on {date}\")\n",
    "    try:\n",
    "        stock_url = f\"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={ticker}&apikey={STOCK_API_KEY}\"\n",
    "        stock_data = requests.get(stock_url)\n",
    "        stock_low = stock_data.json()[\"Time Series (Daily)\"][date][\"3. low\"]\n",
    "        stock_high = stock_data.json()[\"Time Series (Daily)\"][date][\"2. high\"]\n",
    "        return stock_low, stock_high\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching stock data: {e}\")\n",
    "        return \"none\", \"none\"\n",
    "    \n",
    "    \n",
    "available_function_calls = {\"get_current_date\": get_current_date, \"get_current_weather\": get_current_weather, \"celsius_to_fahrenheit\": celsius_to_fahrenheit,\n",
    "                      \"get_nasa_picture_of_the_day\": get_nasa_picture_of_the_day, \"get_stock_price\": get_stock_price}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "87fb19b5c3c24c1098d06ad4965594cb"
   },
   "outputs": [],
   "source": [
    "functions = [\n",
    "    {\n",
    "        \"name\": \"get_current_date\",\n",
    "        \"description\": \"Fetches the current date in the format YYYY-MM-DD.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {},\n",
    "            \"required\": [],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get the current weather\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city and country code, e.g. San Francisco, US\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"location\"],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"celsius_to_fahrenheit\",\n",
    "        \"description\": \"Converts a temperature from Celsius to Fahrenheit.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"celsius\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"Temperature in degrees Celsius.\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"celsius\"],\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"get_nasa_picture_of_the_day\",\n",
    "        \"description\": \"Fetches NASA's Picture of the Day information for a given date.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"date\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Date in YYYY-MM-DD format for which to retrieve the picture.\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"date\"],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"get_stock_price\",\n",
    "        \"description\": \"Retrieves the lowest and highest stock price for a given ticker symbol and date. The ticker symbol must be a valid symbol for a publicly traded company on a major US stock exchange like NYSE or NASDAQ. The tool will return the latest trade price in USD.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"ticker\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The stock ticker symbol, e.g. AAPL for Apple Inc.\",\n",
    "                },\n",
    "                \"date\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Date in YYYY-MM-DD format\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"ticker\", \"date\"],\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "available_tools_list = {\n",
    "    \"functions_str\": [json.dumps(x) for x in functions],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "17509ce850a44ea3961c039dc875a99f"
   },
   "outputs": [],
   "source": [
    "query = \"What is the current weather at the headquarters of IBM? Also, can you provide the stock prices for the company on October 29, 2024?\"\n",
    "\n",
    "chat = [\n",
    "    {\"role\":\"system\",\"content\": f\"You are a helpful assistant with access to the following function calls. Your task is to produce a sequence of function calls necessary to generate response to the user utterance. Use the following function calls as required.\\n{available_tools_list}\"},\n",
    "    {\"role\": \"user\", \"content\": query }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "87635067c20444fc8973c010076ca141"
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer.apply_chat_template(\n",
    "    chat,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "f53a91f537b240bda2d7cd8e900640ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful assistant with access to the following function calls. Your task is to produce a sequence of function calls necessary to generate response to the user utterance. Use the following function calls as required.\n",
      "{'functions_str': ['{\"name\": \"get_current_date\", \"description\": \"Fetches the current date in the format YYYY-MM-DD.\", \"parameters\": {\"type\": \"object\", \"properties\": {}, \"required\": []}}', '{\"name\": \"get_current_weather\", \"description\": \"Get the current weather\", \"parameters\": {\"type\": \"object\", \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"The city and country code, e.g. San Francisco, US\"}}, \"required\": [\"location\"]}}', '{\"name\": \"celsius_to_fahrenheit\", \"description\": \"Converts a temperature from Celsius to Fahrenheit.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"celsius\": {\"type\": \"number\", \"description\": \"Temperature in degrees Celsius.\"}}, \"required\": [\"celsius\"]}}', '{\"name\": \"get_nasa_picture_of_the_day\", \"description\": \"Fetches NASA\\'s Picture of the Day information for a given date.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"date\": {\"type\": \"string\", \"description\": \"Date in YYYY-MM-DD format for which to retrieve the picture.\"}}, \"required\": [\"date\"]}}', '{\"name\": \"get_stock_price\", \"description\": \"Retrieves the lowest and highest stock price for a given ticker symbol and date. The ticker symbol must be a valid symbol for a publicly traded company on a major US stock exchange like NYSE or NASDAQ. The tool will return the latest trade price in USD.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"ticker\": {\"type\": \"string\", \"description\": \"The stock ticker symbol, e.g. AAPL for Apple Inc.\"}, \"date\": {\"type\": \"string\", \"description\": \"Date in YYYY-MM-DD format\"}}, \"required\": [\"ticker\", \"date\"]}}']}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the current weather at the headquarters of IBM? Also, can you provide the stock prices for the company on October 29, 2024?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "[{\"name\": \"get_current_weather\", \"arguments\": {\"location\": \"Armonk, US\"}}, {\"name\": \"get_stock_price\", \"arguments\": {\"ticker\": \"IBM\", \"date\": \"2024-10-29\"}}]<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(input_ids = inputs, max_new_tokens = 1024, use_cache = True)\n",
    "response = tokenizer.batch_decode(outputs)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "dd8e9e00fa6f475480fa43cf44e3c5be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"name\": \"get_current_weather\", \"arguments\": {\"location\": \"Armonk, US\"}}, {\"name\": \"get_stock_price\", \"arguments\": {\"ticker\": \"IBM\", \"date\": \"2024-10-29\"}}]<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 512, use_cache = True, pad_token_id = tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "7ea28b26483b4eca83f829aa01772dc6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'get_current_weather', 'arguments': {'location': 'Armonk, US'}},\n",
       " {'name': 'get_stock_price',\n",
       "  'arguments': {'ticker': 'IBM', 'date': '2024-10-29'}}]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_content(text):\n",
    "    # Define the regex pattern to extract the content\n",
    "    pattern = r\"<\\|start_header_id\\|>assistant<\\|end_header_id\\|>(.*?)<\\|eot_id\\|>\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return None  \n",
    "\n",
    "parsed_response = json.loads(extract_content(response))\n",
    "parsed_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "5bf0168fca39450d8e9fb83a0479364a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting current weather for Armonk, US\n",
      "{'description': 'clear sky', 'temperature': 11.23, 'humidity': 37} \n",
      "\n",
      "Getting stock price for IBM on 2024-10-29\n",
      "('230.2600', '232.4200') \n",
      "\n",
      "The current weather at IBM's headquarters in Armonk, USA is a clear sky with a temperature of 11.23 degrees Celsius and a humidity of 37%. Additionally, the stock price for IBM on October 29, 2024 was between $230.26 and $232.42.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "if parsed_response:\n",
    "    new_system_content = \"You are a helpful assistant. Answer the user query based on the response of the specific function call or tool provided to you as context. Generate a precise answer for given user query, synthesizing the provided information.\"\n",
    "\n",
    "    for res in parsed_response:\n",
    "        obtained_function = res.get(\"name\")\n",
    "        arguments = res.get(\"arguments\")\n",
    "        function_description = next(item['description'] for item in functions if item['name'] == obtained_function)\n",
    "        function_to_call = available_function_calls[obtained_function]\n",
    "        response = function_to_call(**arguments)\n",
    "        print(response, \"\\n\")\n",
    "        \n",
    "        chat.append({\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": f\"The tool - '{obtained_function}' with the function definition - '{function_description}' and function arguments - '{arguments}' yielded the following response: {response}\\n.\"\n",
    "        })\n",
    "\n",
    "        for message in chat:\n",
    "            if message['role'] == 'system':\n",
    "                message['content'] = new_system_content\n",
    "                \n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        chat,\n",
    "        tokenize = True,\n",
    "        add_generation_prompt = True, \n",
    "        return_tensors = \"pt\").to(\"cuda\")\n",
    "    text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "    _ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 512, use_cache = True, pad_token_id = tokenizer.eos_token_id)\n",
    "else:\n",
    "    print(\"No function call found in the response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "0175b656c9684cdf9540e6cdafaa1301"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are a helpful assistant. Answer the user query based on the response of the specific function call or tool provided to you as context. Generate a precise answer for given user query, synthesizing the provided information.'},\n",
       " {'role': 'user',\n",
       "  'content': 'What is the current weather at the headquarters of IBM? Also, can you provide the stock prices for the company on October 29, 2024?'},\n",
       " {'role': 'tool',\n",
       "  'content': \"The tool - 'get_current_weather' with the function definition - 'Get the current weather' and function arguments = '{'location': 'Armonk, US'}' yielded the following response: {'description': 'clear sky', 'temperature': 11.23, 'humidity': 37}\\n.\"},\n",
       " {'role': 'tool',\n",
       "  'content': \"The tool - 'get_stock_price' with the function definition - 'Retrieves the lowest and highest stock price for a given ticker symbol and date. The ticker symbol must be a valid symbol for a publicly traded company on a major US stock exchange like NYSE or NASDAQ. The tool will return the latest trade price in USD.' and function arguments = '{'ticker': 'IBM', 'date': '2024-10-29'}' yielded the following response: ('230.2600', '232.4200')\\n.\"}]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "f5cbef087a834afc8fc8b1b52f2eafd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful assistant. Answer the user query based on the response of the specific function call or tool provided to you as context. Generate a precise answer for given user query, synthesizing the provided information.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the current weather at the headquarters of IBM? Also, can you provide the stock prices for the company on October 29, 2024?<|eot_id|><|start_header_id|>tool<|end_header_id|>\n",
      "\n",
      "The tool - 'get_current_weather' with the function definition - 'Get the current weather' and function arguments = '{'location': 'Armonk, US'}' yielded the following response: {'description': 'clear sky', 'temperature': 11.23, 'humidity': 37}\n",
      ".<|eot_id|><|start_header_id|>tool<|end_header_id|>\n",
      "\n",
      "The tool - 'get_stock_price' with the function definition - 'Retrieves the lowest and highest stock price for a given ticker symbol and date. The ticker symbol must be a valid symbol for a publicly traded company on a major US stock exchange like NYSE or NASDAQ. The tool will return the latest trade price in USD.' and function arguments = '{'ticker': 'IBM', 'date': '2024-10-29'}' yielded the following response: ('230.2600', '232.4200')\n",
      ".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "formatted_prompt = tokenizer.apply_chat_template(\n",
    "    chat,\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, \n",
    "    return_tensors = \"pt\"\n",
    ")\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "df58388f93f44f8f93ea54e1a479d465"
   },
   "source": [
    "## vLLM Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7429d34d915b4d438dc93d459235bfbc"
   },
   "outputs": [],
   "source": [
    "from vllm import LLM\n",
    "from vllm.sampling_params import SamplingParams\n",
    "\n",
    "model_name = \"gautamgc17/Llama3-FineTune-Function-Calling-Model-V2\"\n",
    "sampling_params = SamplingParams(max_tokens=768)\n",
    "\n",
    "llm = LLM(\n",
    "    model=model_name,\n",
    "    max_model_len=2048,\n",
    "    tokenizer_mode=\"auto\",\n",
    "    tensor_parallel_size=1,\n",
    "    enforce_eager=True,\n",
    "    gpu_memory_utilization=0.95\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "5f4241ad9fe64115850bacb5e1b328ec"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.25s/it, est. speed input: 386.51 toks/s, output: 42.50 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: '[{\"name\": \"get_current_weather\", \"arguments\": {\"location\": \"Armonk, US\"}}, {\"name\": \"get_stock_price\", \"arguments\": {\"ticker\": \"IBM\", \"date\": \"2024-10-29\"}}]'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llm_prompt = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a helpful assistant with access to the following function calls. Your task is to produce a sequence of function calls necessary to generate response to the user utterance. Use the following function calls as required.\n",
    "{available_tools_list}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{query}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "input_prompt = llm_prompt.format(available_tools_list=available_tools_list, query=query)\n",
    "\n",
    "output = llm.generate([input_prompt], sampling_params)\n",
    "generated_text = output[0].outputs[0].text\n",
    "print(f\"Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "244ff0273c3e48758385a37bed70eec9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
