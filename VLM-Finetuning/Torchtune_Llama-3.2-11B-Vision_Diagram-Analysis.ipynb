{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5e3c23fae4364b26a0fe5161ede2cb54"
   },
   "source": [
    "## Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "485846c0-c42b-4a46-88e4-ae6956eb9119"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from pandas->datasets->torchtune) (2022.7)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchao torchtune wandb --upgrade --no-cache | tail -n 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a94f60b2ef0648948dd39caca69505db"
   },
   "source": [
    "## Explore torchtune commands\n",
    "\n",
    "Check available commands in TorchTune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1b671365139749288b16eb88a527e53c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: tune [-h] {download,ls,cp,run,validate} ...\r\n",
      "\r\n",
      "Welcome to the torchtune CLI!\r\n",
      "\r\n",
      "options:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "\r\n",
      "subcommands:\r\n",
      "  {download,ls,cp,run,validate}\r\n",
      "    download            Download a model from the Hugging Face Hub.\r\n",
      "    ls                  List all built-in recipes and configs\r\n",
      "    cp                  Copy a built-in recipe or config to a local path.\r\n",
      "    run                 Run a recipe. For distributed recipes, this supports\r\n",
      "                        all torchrun arguments.\r\n",
      "    validate            Validate a config and ensure that it is well-formed.\r\n"
     ]
    }
   ],
   "source": [
    "!tune --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8d6346b554e64de7a3b27e60384069d0"
   },
   "source": [
    "## List all built-in recipes and configs\n",
    "\n",
    "To view different finetuning recipes and the associated configs. Each recipe consists of three components:\n",
    "\n",
    "- Configurable parameters, specified through yaml configs and command-line overrides\n",
    "\n",
    "- Recipe Script, entry-point which puts everything together including parsing and validating configs, setting up the environment, and correctly using the recipe class\n",
    "\n",
    "- Recipe Class, core logic needed for training, exposed to users through a set of APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2d1e16e324734cff869caea5b355ef3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RECIPE                                   CONFIG                                  \r\n",
      "full_finetune_single_device              llama2/7B_full_low_memory               \r\n",
      "                                         code_llama2/7B_full_low_memory          \r\n",
      "                                         llama3/8B_full_single_device            \r\n",
      "                                         llama3_1/8B_full_single_device          \r\n",
      "                                         llama3_2/1B_full_single_device          \r\n",
      "                                         llama3_2/3B_full_single_device          \r\n",
      "                                         mistral/7B_full_low_memory              \r\n",
      "                                         phi3/mini_full_low_memory               \r\n",
      "                                         qwen2/7B_full_single_device             \r\n",
      "                                         qwen2/0.5B_full_single_device           \r\n",
      "                                         qwen2/1.5B_full_single_device           \r\n",
      "                                         qwen2_5/0.5B_full_single_device         \r\n",
      "                                         qwen2_5/1.5B_full_single_device         \r\n",
      "                                         qwen2_5/3B_full_single_device           \r\n",
      "                                         qwen2_5/7B_full_single_device           \r\n",
      "                                         llama3_2_vision/11B_full_single_device  \r\n",
      "full_finetune_distributed                llama2/7B_full                          \r\n",
      "                                         llama2/13B_full                         \r\n",
      "                                         llama3/8B_full                          \r\n",
      "                                         llama3_1/8B_full                        \r\n",
      "                                         llama3_2/1B_full                        \r\n",
      "                                         llama3_2/3B_full                        \r\n",
      "                                         llama3/70B_full                         \r\n",
      "                                         llama3_1/70B_full                       \r\n",
      "                                         mistral/7B_full                         \r\n",
      "                                         gemma/2B_full                           \r\n",
      "                                         gemma/7B_full                           \r\n",
      "                                         phi3/mini_full                          \r\n",
      "                                         qwen2/7B_full                           \r\n",
      "                                         qwen2/0.5B_full                         \r\n",
      "                                         qwen2/1.5B_full                         \r\n",
      "                                         qwen2_5/0.5B_full                       \r\n",
      "                                         qwen2_5/1.5B_full                       \r\n",
      "                                         qwen2_5/3B_full                         \r\n",
      "                                         qwen2_5/7B_full                         \r\n",
      "                                         llama3_2_vision/11B_full                \r\n",
      "                                         llama3_2_vision/90B_full                \r\n",
      "lora_finetune_single_device              llama2/7B_lora_single_device            \r\n",
      "                                         llama2/7B_qlora_single_device           \r\n",
      "                                         code_llama2/7B_lora_single_device       \r\n",
      "                                         code_llama2/7B_qlora_single_device      \r\n",
      "                                         llama3/8B_lora_single_device            \r\n",
      "                                         llama3_1/8B_lora_single_device          \r\n",
      "                                         llama3/8B_qlora_single_device           \r\n",
      "                                         llama3_2/1B_lora_single_device          \r\n",
      "                                         llama3_2/3B_lora_single_device          \r\n",
      "                                         llama3/8B_dora_single_device            \r\n",
      "                                         llama3/8B_qdora_single_device           \r\n",
      "                                         llama3_1/8B_qlora_single_device         \r\n",
      "                                         llama3_2/1B_qlora_single_device         \r\n",
      "                                         llama3_2/3B_qlora_single_device         \r\n",
      "                                         llama2/13B_qlora_single_device          \r\n",
      "                                         mistral/7B_lora_single_device           \r\n",
      "                                         mistral/7B_qlora_single_device          \r\n",
      "                                         gemma/2B_lora_single_device             \r\n",
      "                                         gemma/2B_qlora_single_device            \r\n",
      "                                         gemma/7B_lora_single_device             \r\n",
      "                                         gemma/7B_qlora_single_device            \r\n",
      "                                         phi3/mini_lora_single_device            \r\n",
      "                                         phi3/mini_qlora_single_device           \r\n",
      "                                         qwen2/7B_lora_single_device             \r\n",
      "                                         qwen2/0.5B_lora_single_device           \r\n",
      "                                         qwen2/1.5B_lora_single_device           \r\n",
      "                                         qwen2_5/0.5B_lora_single_device         \r\n",
      "                                         qwen2_5/1.5B_lora_single_device         \r\n",
      "                                         qwen2_5/3B_lora_single_device           \r\n",
      "                                         qwen2_5/7B_lora_single_device           \r\n",
      "                                         qwen2_5/14B_lora_single_device          \r\n",
      "                                         llama3_2_vision/11B_lora_single_device  \r\n",
      "                                         llama3_2_vision/11B_qlora_single_device \r\n",
      "lora_dpo_single_device                   llama2/7B_lora_dpo_single_device        \r\n",
      "lora_dpo_distributed                     llama2/7B_lora_dpo                      \r\n",
      "ppo_full_finetune_single_device          mistral/7B_full_ppo_low_memory          \r\n",
      "lora_finetune_distributed                llama2/7B_lora                          \r\n",
      "                                         llama2/13B_lora                         \r\n",
      "                                         llama2/70B_lora                         \r\n",
      "                                         llama2/7B_qlora                         \r\n",
      "                                         llama2/70B_qlora                        \r\n",
      "                                         llama3/8B_dora                          \r\n",
      "                                         llama3/70B_lora                         \r\n",
      "                                         llama3_1/70B_lora                       \r\n",
      "                                         llama3/8B_lora                          \r\n",
      "                                         llama3_1/8B_lora                        \r\n",
      "                                         llama3_2/1B_lora                        \r\n",
      "                                         llama3_2/3B_lora                        \r\n",
      "                                         llama3_1/405B_qlora                     \r\n",
      "                                         mistral/7B_lora                         \r\n",
      "                                         gemma/2B_lora                           \r\n",
      "                                         gemma/7B_lora                           \r\n",
      "                                         phi3/mini_lora                          \r\n",
      "                                         qwen2/7B_lora                           \r\n",
      "                                         qwen2/0.5B_lora                         \r\n",
      "                                         qwen2/1.5B_lora                         \r\n",
      "                                         qwen2_5/0.5B_lora                       \r\n",
      "                                         qwen2_5/1.5B_lora                       \r\n",
      "                                         qwen2_5/3B_lora                         \r\n",
      "                                         qwen2_5/7B_lora                         \r\n",
      "                                         qwen2_5/32B_lora                        \r\n",
      "                                         qwen2_5/72B_lora                        \r\n",
      "                                         llama3_2_vision/11B_lora                \r\n",
      "                                         llama3_2_vision/11B_qlora               \r\n",
      "                                         llama3_2_vision/90B_lora                \r\n",
      "                                         llama3_2_vision/90B_qlora               \r\n",
      "generate                                 generation                              \r\n",
      "dev/generate_v2                          llama2/generation_v2                    \r\n",
      "                                         llama3_2_vision/11B_generation_v2       \r\n",
      "eleuther_eval                            eleuther_evaluation                     \r\n",
      "                                         llama3_2_vision/11B_evaluation          \r\n",
      "                                         qwen2/evaluation                        \r\n",
      "                                         gemma/evaluation                        \r\n",
      "                                         phi3/evaluation                         \r\n",
      "                                         mistral/evaluation                      \r\n",
      "quantize                                 quantization                            \r\n",
      "qat_distributed                          llama2/7B_qat_full                      \r\n",
      "                                         llama3/8B_qat_full                      \r\n",
      "knowledge_distillation_single_device     qwen2/knowledge_distillation_single_device\r\n",
      "                                         llama3_2/knowledge_distillation_single_device\r\n",
      "knowledge_distillation_distributed       qwen2/knowledge_distillation_distributed\r\n",
      "                                         llama3_2/knowledge_distillation_distributed\r\n"
     ]
    }
   ],
   "source": [
    "!tune ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "da1418519bbe4adb8fed45f6c65e383b"
   },
   "source": [
    "## Download Llama3.2-11B-Vision-Instruct model\n",
    "\n",
    "Prepare a folder to store the downloaded model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "2e23bf9cc1b046ce868a416d5577d2b6"
   },
   "outputs": [],
   "source": [
    "!mkdir /tmp/Llama-3.2-11B-Vision-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c70140e2e30441c584bde09d02835d34"
   },
   "source": [
    "The below command will download the model, along with the tokenizer, which is necessary for processing both the image and text inputs. After downloading the model, you can start setting up the fine-tuning configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "75d9ba5d463347cc82d8295b1c24d917"
   },
   "outputs": [],
   "source": [
    "!tune download meta-llama/Llama-3.2-11B-Vision-Instruct --output-dir /tmp/Llama-3.2-11B-Vision-Instruct --ignore-patterns \"original/consolidated*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ae47e0efea14f50861e0a02883c365c"
   },
   "source": [
    "## Configure Fine-Tuning Setup\n",
    "\n",
    "We will fine-tune model with LoRA configuration on a single device.\n",
    "\n",
    "#### Modify config\n",
    "Copy the existing configuration file template for your fine-tuning setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "96a00a6fde7a4e5a977ceb5733d99a04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied file to custom_config_file.yaml\r\n"
     ]
    }
   ],
   "source": [
    "!tune cp llama3_2_vision/11B_qlora_single_device ./custom_config_file.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "065a1e1e0c59455d8b49ce8f2d9a7faa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File custom_config_file.yaml saved successfully.\n"
     ]
    }
   ],
   "source": [
    "from ibm_watson_studio_lib import access_project_or_space\n",
    "\n",
    "# Access your project or space\n",
    "wslib = access_project_or_space()\n",
    "\n",
    "# Open the existing YAML file and read its content\n",
    "file_path = 'custom_config_file.yaml'  \n",
    "with open(file_path, 'r') as file:\n",
    "    file_data = file.read()\n",
    "\n",
    "file_data_bytes = file_data.encode('utf-8')\n",
    "wslib.save_data(file_path, file_data_bytes)\n",
    "print(f\"File {file_path} saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b72a32432d6f43f58daddeff619e5202"
   },
   "outputs": [],
   "source": [
    "## Edit the following fields:\n",
    "\n",
    "\"\"\"\n",
    "# Dataset\n",
    "dataset:\n",
    "  _component_: torchtune.datasets.multimodal.the_cauldron_dataset\n",
    "  subset: diagram_image_to_text\n",
    "seed: null\n",
    "shuffle: True\n",
    "collate_fn: torchtune.data.padded_collate_tiled_images_and_mask\n",
    "\n",
    "\n",
    "# Fine-tuning arguments\n",
    "epochs: 3\n",
    "\n",
    "\n",
    "# Logging\n",
    "output_dir: /tmp/lora-llama3.2-vision-finetune\n",
    "metric_logger:\n",
    "  _component_: torchtune.utils.metric_logging.WandBLogger\n",
    "  project: llama-3.2-vlm-torchtune\n",
    "log_every_n_steps: 1\n",
    "log_peak_memory_stats: True\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3089032cac4a4bc28795e6c4717197d0"
   },
   "source": [
    "#### Validate Configuration\n",
    "Ensure that the configuration file is properly set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "89ba0f6164564676807f2497512c8c80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config is well-formed!\r\n"
     ]
    }
   ],
   "source": [
    "!tune validate /project_data/data_asset/custom_config_file.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "8681507344f8442b84086a78be417905"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Config for single device QLoRA finetuning in lora_finetune_single_device.py\n",
      "# using a Llama3.2 11B Vision Instruct model\n",
      "#\n",
      "# This config assumes that you've run the following command before launching:\n",
      "#   tune download meta-llama/Llama-3.2-11B-Vision-Instruct --output-dir /tmp/Llama-3.2-11B-Vision-Instruct --ignore-patterns \"original/consolidated*\"\n",
      "#\n",
      "# To launch on a single device, run the following command from root:\n",
      "#   tune run lora_finetune_single_device --config llama3_2_vision/11B_qlora_single_device\n",
      "#\n",
      "# You can add specific overrides through the command line. For example\n",
      "# to override the checkpointer directory while launching training:\n",
      "#   tune run lora_finetune_single_device --config llama3_2_vision/11B_qlora_single_device checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>\n",
      "#\n",
      "# This config works only for training on single device.\n",
      "\n",
      "# Model arguments\n",
      "model:\n",
      "  _component_: torchtune.models.llama3_2_vision.qlora_llama3_2_vision_11b\n",
      "  decoder_trainable: \"frozen\"\n",
      "  encoder_trainable: \"lora\"\n",
      "  fusion_trainable: \"lora\"\n",
      "  lora_attn_modules: ['q_proj', 'v_proj', 'output_proj']\n",
      "  apply_lora_to_mlp: True\n",
      "  apply_lora_to_output: False\n",
      "  lora_rank: 8  # higher increases accuracy and memory\n",
      "  lora_alpha: 16  # usually alpha=2*rank\n",
      "  lora_dropout: 0.0\n",
      "  image_size: 560 # Make sure this matches the image_size in tokenizer\n",
      "\n",
      "# Transform\n",
      "tokenizer:\n",
      "  _component_: torchtune.models.llama3_2_vision.llama3_2_vision_transform\n",
      "  path: /tmp/Llama-3.2-11B-Vision-Instruct/original/tokenizer.model\n",
      "  image_size: 560\n",
      "  max_seq_len: 8192\n",
      "\n",
      "# Checkpointer\n",
      "checkpointer:\n",
      "  _component_: torchtune.training.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /tmp/Llama-3.2-11B-Vision-Instruct/\n",
      "  checkpoint_files:\n",
      "    filename_format: model-{}-of-{}.safetensors\n",
      "    max_filename: \"00005\"\n",
      "  recipe_checkpoint: null\n",
      "  output_dir: /tmp/Llama-3.2-11B-Vision-Instruct/\n",
      "  model_type: LLAMA3_VISION\n",
      "resume_from_checkpoint: False\n",
      "save_adapter_weights_only: False # PeFT formatting not available yet. This will save it in torchtune format only.\n",
      "\n",
      "# Dataset\n",
      "dataset:\n",
      "  _component_: torchtune.datasets.multimodal.the_cauldron_dataset\n",
      "  subset: diagram_image_to_text\n",
      "seed: null\n",
      "shuffle: True\n",
      "collate_fn: torchtune.data.padded_collate_tiled_images_and_mask\n",
      "\n",
      "# Fine-tuning arguments\n",
      "epochs: 1\n",
      "max_steps_per_epoch: null\n",
      "batch_size: 2\n",
      "gradient_accumulation_steps: 8  # Use to increase virtual batch size\n",
      "optimizer:\n",
      "  _component_: torch.optim.AdamW\n",
      "  fused: True\n",
      "  weight_decay: 0.01\n",
      "  lr: 1e-4\n",
      "\n",
      "lr_scheduler:\n",
      "  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup\n",
      "  num_warmup_steps: 100\n",
      "loss:\n",
      "  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss\n",
      "clip_grad_norm: 1.0\n",
      "compile: False  # pytorch compile, set to true for better perf/memory\n",
      "\n",
      "# Training env\n",
      "device: cuda\n",
      "\n",
      "# Memory management\n",
      "enable_activation_checkpointing: True  # True reduces memory\n",
      "dtype: bf16\n",
      "\n",
      "# Logging\n",
      "output_dir: /tmp/qlora-llama3.2-vision-finetune\n",
      "metric_logger:\n",
      "  _component_: torchtune.training.metric_logging.WandBLogger\n",
      "  project: llama-3.2-vlm-torchtune\n",
      "log_every_n_steps: 1\n",
      "log_peak_memory_stats: True\n",
      "\n",
      "# Profiler (disabled)\n",
      "profiler:\n",
      "  _component_: torchtune.training.setup_torch_profiler\n",
      "  enabled: False\n",
      "\n",
      "  #Output directory of trace artifacts\n",
      "  output_dir: ${output_dir}/profiling_outputs\n",
      "\n",
      "  #`torch.profiler.ProfilerActivity` types to trace\n",
      "  cpu: True\n",
      "  cuda: True\n",
      "\n",
      "  #trace options passed to `torch.profiler.profile`\n",
      "  profile_memory: False\n",
      "  with_stack: False\n",
      "  record_shapes: True\n",
      "  with_flops: False\n",
      "\n",
      "  # `torch.profiler.schedule` options:\n",
      "  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat\n",
      "  wait_steps: 5\n",
      "  warmup_steps: 3\n",
      "  active_steps: 2\n",
      "  num_cycles: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Open and read the YAML file\n",
    "file_path = '/project_data/data_asset/custom_config_file.yaml'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89678936dd844236a03982631b595cdc"
   },
   "source": [
    "#### Set Up Weights & Biases (W&B) for Logging\n",
    "Log into your W&B account to enable training tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "502b7a086a2946a083c3d666ac06d887"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/wsuser/.netrc\r\n"
     ]
    }
   ],
   "source": [
    "# !wandb login <API_KEY>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "765873120782406dadb89ed40feb5d94"
   },
   "source": [
    "## Running the Fine-Tuning Script\n",
    "\n",
    "Once the configuration is set, you can run the fine-tuning process using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "2d87bba252fc4d31ac986ceec5ce2b11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:torchtune.utils._logging:Running LoRAFinetuneRecipeSingleDevice with resolved config:\n",
      "\n",
      "batch_size: 2\n",
      "checkpointer:\n",
      "  _component_: torchtune.training.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /tmp/Llama-3.2-11B-Vision-Instruct/\n",
      "  checkpoint_files:\n",
      "    filename_format: model-{}-of-{}.safetensors\n",
      "    max_filename: '00005'\n",
      "  model_type: LLAMA3_VISION\n",
      "  output_dir: /tmp/Llama-3.2-11B-Vision-Instruct/\n",
      "  recipe_checkpoint: null\n",
      "clip_grad_norm: 1.0\n",
      "collate_fn: torchtune.data.padded_collate_tiled_images_and_mask\n",
      "compile: false\n",
      "dataset:\n",
      "  _component_: torchtune.datasets.multimodal.the_cauldron_dataset\n",
      "  subset: diagram_image_to_text\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_activation_checkpointing: true\n",
      "epochs: 1\n",
      "gradient_accumulation_steps: 8\n",
      "log_every_n_steps: 1\n",
      "log_peak_memory_stats: true\n",
      "loss:\n",
      "  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss\n",
      "lr_scheduler:\n",
      "  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup\n",
      "  num_warmup_steps: 100\n",
      "max_steps_per_epoch: null\n",
      "metric_logger:\n",
      "  _component_: torchtune.training.metric_logging.WandBLogger\n",
      "  project: llama-3.2-vlm-torchtune\n",
      "model:\n",
      "  _component_: torchtune.models.llama3_2_vision.qlora_llama3_2_vision_11b\n",
      "  apply_lora_to_mlp: true\n",
      "  apply_lora_to_output: false\n",
      "  decoder_trainable: frozen\n",
      "  encoder_trainable: lora\n",
      "  fusion_trainable: lora\n",
      "  image_size: 560\n",
      "  lora_alpha: 16\n",
      "  lora_attn_modules:\n",
      "  - q_proj\n",
      "  - v_proj\n",
      "  - output_proj\n",
      "  lora_dropout: 0.0\n",
      "  lora_rank: 8\n",
      "optimizer:\n",
      "  _component_: torch.optim.AdamW\n",
      "  fused: true\n",
      "  lr: 0.0001\n",
      "  weight_decay: 0.01\n",
      "output_dir: /tmp/qlora-llama3.2-vision-finetune\n",
      "profiler:\n",
      "  _component_: torchtune.training.setup_torch_profiler\n",
      "  active_steps: 2\n",
      "  cpu: true\n",
      "  cuda: true\n",
      "  enabled: false\n",
      "  num_cycles: 1\n",
      "  output_dir: /tmp/qlora-llama3.2-vision-finetune/profiling_outputs\n",
      "  profile_memory: false\n",
      "  record_shapes: true\n",
      "  wait_steps: 5\n",
      "  warmup_steps: 3\n",
      "  with_flops: false\n",
      "  with_stack: false\n",
      "resume_from_checkpoint: false\n",
      "save_adapter_weights_only: false\n",
      "seed: null\n",
      "shuffle: true\n",
      "tokenizer:\n",
      "  _component_: torchtune.models.llama3_2_vision.llama3_2_vision_transform\n",
      "  image_size: 560\n",
      "  max_seq_len: 8192\n",
      "  path: /tmp/Llama-3.2-11B-Vision-Instruct/original/tokenizer.model\n",
      "\n",
      "DEBUG:torchtune.utils._logging:Setting manual seed to local seed 359891295. Local seed is seed + rank = 359891295 + 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgautamgc75\u001b[0m (\u001b[33mgautamgc75-org\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/wsuser/work/wandb/run-20241129_204219-hb1oimoe\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33molive-vortex-2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/gautamgc75-org/llama-3.2-vlm-torchtune\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/gautamgc75-org/llama-3.2-vlm-torchtune/runs/hb1oimoe\u001b[0m\n",
      "INFO:torchtune.utils._logging:Logging /tmp/Llama-3.2-11B-Vision-Instruct/torchtune_config.yaml to W&B under Files\n",
      "INFO:torchtune.utils._logging:Model is initialized with precision torch.bfloat16.\n",
      "INFO:torchtune.utils._logging:Memory stats after model init:\n",
      "\tGPU peak memory allocation: 7.12 GiB\n",
      "\tGPU peak memory reserved: 7.47 GiB\n",
      "\tGPU peak memory active: 7.12 GiB\n",
      "INFO:torchtune.utils._logging:Tokenizer is initialized from file.\n",
      "INFO:torchtune.utils._logging:Optimizer and loss are initialized.\n",
      "INFO:torchtune.utils._logging:Loss is initialized.\n",
      "INFO:torchtune.utils._logging:Dataset and Sampler are initialized.\n",
      "INFO:torchtune.utils._logging:Learning rate scheduler is initialized.\n",
      "WARNING:torchtune.utils._logging: Profiling disabled.\n",
      "INFO:torchtune.utils._logging: Profiler config after instantiation: {'enabled': False}\n",
      "1|18|Loss: 2.521054744720459: 100%|█████████████| 18/18 [05:39<00:00, 18.79s/it]INFO:torchtune.utils._logging:Starting checkpoint save...\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.99 GB saved to /tmp/Llama-3.2-11B-Vision-Instruct/hf_model_0001_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.97 GB saved to /tmp/Llama-3.2-11B-Vision-Instruct/hf_model_0002_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.92 GB saved to /tmp/Llama-3.2-11B-Vision-Instruct/hf_model_0003_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 5.00 GB saved to /tmp/Llama-3.2-11B-Vision-Instruct/hf_model_0004_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 1.47 GB saved to /tmp/Llama-3.2-11B-Vision-Instruct/hf_model_0005_0.pt\n",
      "INFO:torchtune.utils._logging:Adapter checkpoint of size 0.06 GB saved to /tmp/Llama-3.2-11B-Vision-Instruct/adapter_0.pt\n",
      "WARNING:torchtune.utils._logging:Saving Llama3.2 Vision adapter weights to PEFT format is not supported, saving to torchtune format instead\n",
      "WARNING:torchtune.utils._logging:PEFT integration for Llama3.2 Vision is not supported, skipping adapter config save\n",
      "INFO:torchtune.utils._logging:Saving final epoch checkpoint.\n",
      "INFO:torchtune.utils._logging:The full model checkpoint, including all weights and configurations, has been saved successfully.You can now use this checkpoint for further training or inference.\n",
      "INFO:torchtune.utils._logging:Checkpoint saved in 48.31 seconds.\n",
      "1|18|Loss: 2.521054744720459: 100%|█████████████| 18/18 [06:42<00:00, 22.36s/it]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               global_step ▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 grad_norm ▆▄▁▆▄▆▄▆▄█▇█▄▆▅▇▅▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      loss █▆▁▇▃▇▂▁▃▅█▇▁▆▃▄▃▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        lr ▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        peak_memory_active ▁▂█▂▃▂▃▃▃▅▂▂▅▃▃▂▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         peak_memory_alloc ▁▂█▂▃▂▃▃▃▅▂▂▅▃▃▂▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      peak_memory_reserved ▁▃████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: tokens_per_second_per_gpu ▃▂█▃▅▅▄▆▇▅▃▁█▁▅▃▅▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               global_step 18\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 grad_norm 2.07812\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      loss 2.52105\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        lr 2e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        peak_memory_active 12.94408\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         peak_memory_alloc 12.94408\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      peak_memory_reserved 14.24023\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: tokens_per_second_per_gpu 58.03911\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33molive-vortex-2\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/gautamgc75-org/llama-3.2-vlm-torchtune/runs/hb1oimoe\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/gautamgc75-org/llama-3.2-vlm-torchtune\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20241129_204219-hb1oimoe/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!tune run lora_finetune_single_device --config /project_data/data_asset/custom_config_file.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d982b71cd2cb485a9ffdd47e0565801f"
   },
   "source": [
    "## Uploading your model to the Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "495ca09ffc9948b48c3588abe9d91545"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.cache',\n",
       " '.gitattributes',\n",
       " 'README.md',\n",
       " 'config.json',\n",
       " 'USE_POLICY.md',\n",
       " 'chat_template.json',\n",
       " 'generation_config.json',\n",
       " 'original',\n",
       " 'LICENSE.txt',\n",
       " 'model.safetensors.index.json',\n",
       " 'preprocessor_config.json',\n",
       " 'special_tokens_map.json',\n",
       " 'tokenizer_config.json',\n",
       " 'tokenizer.json',\n",
       " 'model-00005-of-00005.safetensors',\n",
       " 'model-00002-of-00005.safetensors',\n",
       " 'model-00003-of-00005.safetensors',\n",
       " 'model-00001-of-00005.safetensors',\n",
       " 'model-00004-of-00005.safetensors',\n",
       " 'torchtune_config.yaml',\n",
       " 'logs',\n",
       " 'hf_model_0001_0.pt',\n",
       " 'hf_model_0002_0.pt',\n",
       " 'hf_model_0003_0.pt',\n",
       " 'hf_model_0004_0.pt',\n",
       " 'hf_model_0005_0.pt',\n",
       " 'adapter_0.pt']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(\"/tmp/Llama-3.2-11B-Vision-Instruct/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "89a5ccc0fd8f48df93d21e52bf4cd37a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "The token `model-deploy` has been saved to /home/wsuser/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /home/wsuser/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `model-deploy`\n"
     ]
    }
   ],
   "source": [
    "# !huggingface-cli login --token <HF_TOKEN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "22c1459e076e48739574b3ce366ef810"
   },
   "outputs": [],
   "source": [
    "# !huggingface-cli upload 'hf-repo-id' 'checkpoint-dir'\n",
    "!huggingface-cli upload llama3.2-vlm-torchtune /tmp/Llama-3.2-11B-Vision-Instruct/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cf65b9e3a1204776b03884b07f5c6567"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a47060ee1ccc46b18b4de444219b2625"
   },
   "source": [
    "#### To Load Finetuned model for Inference\n",
    "\n",
    "- https://huggingface.co/docs/transformers/main/en/tasks/image_text_to_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8e55bc7e58564be6a76989778129045a"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
